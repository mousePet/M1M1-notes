\section{Power Series}

\begin{df}
	A polynomial of degree $n$ is a function of the form
	\begin{align*}
	c_0+ c_1 x + c_2 x^2 + \dots + c_n x^2
	\end{align*}
	where $c_0, c_1, \dots , c_n$ are constants, $c_n \neq 0$.
\end{df}

\begin{rk}
It is easy to see that $c_0 = f(0)$. Differentiating gives us
\begin{align*}
f'(x) = c_1 + 2c_2 x + \dots + n c_n x^{n-1}
\end{align*}
And by repeated differentiation we gain
\begin{align*}
c_m = \frac{f^{(m)}(0)}{m!}
\end{align*}
\end{rk}
This gives us the following formula for any polynomial:
\begin{align*}
f(x) = \sum_{m=0}^n \frac{f^{(m)} (0)}{m!} x^m
\end{align*}
If the function is not a polynomial, the formula is evidently not correct but represents an approximation for the function.
This polynomial approximation is called a Maclaurin series. It works near $x=0$ as $f(x)$ and its first $n$ derivatives agree with the polynomial at this value. 

We can shift the expansion point from $x=0$ to another point $a$:
\begin{align*}
S(x) \approx \sum_{m=0}^n \frac{f^{(m)}(a)}{m!} (x-a)^x
\end{align*}
This is now called a Taylor series, approximating $f(x)$ near $x=a$.

The error of the approximation can be specified. In fact, if we write
\begin{align*}
f(x) = \sum_{m=0}^n \frac{f^{(m)}(a)}{m!} (x-a)^x + R_n(x)
\end{align*}
there are the following exact formulas for $R_n(x)$:
\begin{enumerate}
	\item
	Lagrange  form
	\begin{align*}
		R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1}
	\end{align*}
	where $c$ is between $a$ and $x$.
	\item
	Cauchy form
	\begin{align*}
	R_n(x) = \frac{f^{(n+1)}(c)}{n!} (x-a)(x-c)^n
	\end{align*}
	where $c$ is between $a$ and $x$.
	\item
	Integrated form
	\begin{align*}
	R_n(x) = \frac{1}{n!} \int_a^x (x-t)^n f^{(n+1)} (t) dt
	\end{align*}
\end{enumerate}

Summarized this is called the Taylor Theorem:
\begin{tm}
	\emph{Taylor Theorem.} (with Lagrange form of remainder)
	\begin{align*}
	f(x) = \sum_{m=0}^n \frac{f^{(m)}(a)}{m!} (x-a)^m + R_n(x)
	\end{align*}
	where
	\begin{align*}
	R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1}
	\end{align*}
\end{tm}

For the derivation let us first consider Rolle's theorem:

\begin{tm}
	\emph{Rolle's Theorem.} \\
	Suppose $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$ with $f(a) = f(b)$. Then there is a $c \in (a,b)$ such that $f'(c) =0$.
\end{tm}

Since the proof for this requires the \emph{intermediate value theorem} whose proof would lead to a long chain of required theory which has not been dealt with in this course, we assume Rolle's Theorem to be obvious.

A generalization of Rolle's Theorem is the mean value theorem:
\begin{tm}
	\emph{Mean Value Theorem.} (MVT) \\
	Suppose $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$. Then there is a $c \in (a,b)$ such that 
	\begin{align*}
	f'(c) = \frac{f(b) - f(a)}{b-a}
	\end{align*}
\end{tm}

\begin{proof}
	Define the function $g$ with
	\begin{align*}
	g(x) = f(x) - \frac{f(b)-f(a)}{b-a}(x-a)
	\end{align*}
	Hence,
	\begin{align*}
	g(a) = g(b) = f(a) \qquad \wedge \qquad g'(x) = f'(x) - \frac{f(b)- f(a)}{b-a}
	\end{align*}
	Therefore, we can apply Rolle's theorem on $g$. I.e. there exists a $c \in (a,b)$ with 
	\begin{align*}
	& & g(c) & = 0 = f'(c) - \frac{f(b)- f(a)}{b-a} \\
	& \Rightarrow & f'(c) & = \frac{f(b)- f(a)}{b-a}
	\end{align*}
\end{proof}

Having dealt with these basic properties, we can proof the different forms of the remainders at least for $n=0$. 

Then our approximation is 
\begin{align*}
f(x) = f(a) + R_0(x)
\end{align*}
The integral form of $R_0(x)$ is 
\begin{align*}
R_0(x) = \int_a^x f'(t) dt = f(x) - f(a)
\end{align*}
which obviously fits into our formula.

The Lagrange and Cauchy form are the same:
\begin{align*}
f(x) = f(a) + f'(c)(x-a)
\end{align*}
where $c$ lies between $a$ and $c$. The respective $c$ exists according to the MVT.

\begin{comment}
For simplicity let us only proof the Taylor theorem with the Cauchy form of remainder: 
\begin{proof}
\begin{align*}
f(x) & = f(a) + (x-a) f(a) + \dots + \frac{(x-a)^n}{n!} f^{(n)} (a)  + R_n(x)\\
R_n(x) & = \frac{f^{(n+1)}(c) (x-a)(x-c)}{n!}
\end{align*}
It is useful to switch $a$ and $c$ 
\begin{align*}
f(a) = f(x) + (a-x)f(x) + \dots + \frac{(a-x)^n}{n!} f^{(n)}(x) + R_n(a) \\
R_n(a) = f^{(n+1)} \frac {c(a-x)(a-c)}{n!}
\end{align*}
Apply MVT to 
\begin{align*}
h(x) = f(x) + (a-x) f(x) + \dots + \frac{(a-x)^n}{n!} f^{(n)} (x) \\
h'(x) = (a-x) f''(x) + \frac{(a-x)^n}{n!} f^{(n+1)} (x)
\end{align*}
Apply MVT
\begin{align*}
h'c & = \frac{h(a) -h(x)}{a-x} \\
\frac{(a-c)^n}{n!}f^{(n+1)} (c) & = \frac{h(a) -h(x)}{a-x} \\
h(a) & = h(x) + \frac{(a-x)(a-c)}{n!} f^{(n+1)} (c) \\
f(a) & = h(x) + R_n(a)
\end{align*}
\end{proof}

Lagrange form:
Cauchy Mean Value Theorem (Generalization of the standard MVT).
\begin{align*}
\frac{f(b)-f(a)}{g(b)-g(a)} = \frac{f'(c)}{g'(c)} 
\end{align*}
\end{comment}


\begin{ex}
	\begin{itemize}
		\item Consider 
	\begin{align*}
	f(x) & = e^x \\
	f^{(n)}(x) & = e^x \\
	f^{(n)}(0) & = 1
	\end{align*}
	We approximate about $x=0$ with $n=3$ (Maclaurin series):
	\begin{align*}
	f(x) & = 1 +x + \frac{x^2}{3!} + \frac{x^3}{3!} + R_3(x) \\
	R_3(x) & = \frac{e^c x^4}{4!}, & 0 \le c \le x 
	\end{align*}
	Suppose $x$ is negative so $c$ is negative. Then $|e^c| <1$. Hence,
	\begin{align*}
	\left| e^x - \left( 1+x + \frac{x^2}{2!} + \frac{x^3}{3!} \right) \right| < \frac{x^4}{4!}
	\end{align*}
	\item
	For the trigonometrical function this works a bit better:
	\begin{align*}
	f(x) & =  \sin x \\
	f'(x) & = \cos x \\
	f''(x) & = -sin x \\
	f'''(x) & = -\cos x \\
	f^{(4)} & = \sin x
	\end{align*}
	Choose $a = 0$ as the expansion point.
	\begin{align*}
	f^{(2)} (0) = f^{(2)} (0) = f^{(2)} (0) = 0 \\
	f^{(1)} (0) = f^{(5)} (0) =1 \\
	f^{(3)} (0) = f^{(8)} (0) = -1
	\end{align*}
	Apply the result with $n = 4$ 
	\begin{align*}
	\sin x & = x - \frac{x^3}{3!} + R_4(x) \\
	R(4) (x) & = \frac{f^{(5)} (c) x^5}{5!} \\ 
	& = \frac{x^5 \cos x}{5!} 
	\end{align*}
	but $|\cos c | \le 1$.
	\begin{align*}
	|R_4(x)| = \left| \sin x - \left( x - \frac {x^3}{3!} \right) \right| \le \frac{|x|^5}{5!}
	\end{align*}
	This is true for all $x$. Thus, for $x$ approaching 1, we get the approximation
	\begin{align*}
	\sin x \approx x - \frac{x^3}{6}
	\end{align*}
	\end{itemize}
\end{ex}
\subsection{Infinite Taylor series}
\begin{align*}
f(x) = \sum_{m=0}^n \frac{f^{(m)}(a)}{m!} (x-a)^m + R_n(x)
\end{align*}

In some cases $\lim_{n\to \infty} R_n(x) = 0$ for some or all $x$ ($x$ fixed when taking $n \to \infty$ limit). 
In this case 
\begin{align*}
f(x) = \lim_{n \to \infty} \sum_{m=0}^\infty \frac{f^{(m)}(a)}{m!} (x-a)^m
\end{align*}

\begin{ex}
	\begin{itemize}
		\item
		Again, consider
	\begin{align*}
	f(x) & = e^x \\
	f^{(m)}(x) & = e^x \\
	f^{(m)}(0) & = 1
	\end{align*}
	Expand about $a=0$:
	\begin{align*}
	f(x) & =  \sum_{m=0}^n \frac{f^{(m)}(a)}{m!} +R_n(x) \\
	R_n(x) & = \frac{f^{(n+1)}(c)}{(n+1)!} x^{n+1} = \frac{e^cx^{n+1}}{(n+1)!}, \qquad 0 \le c \le x
	\end{align*}
	We claim $R_n(x) \to 0$ as $n \to \infty$ for any fixed $x$. For example let us consider $x= 1000$:
	\begin{align*}
	R_n & = \frac{e^c 1000^{n+1}}{(n+1)!} \\
	& \le \frac{e^{1000} 10^{3(n+1)}}{(n+1)!} \\
	& \overset{n \to \infty}{\to} 0
	\end{align*}
	(Factorials always grow faster than exponentials.)
	In this example $R_n(x) \to 0$ as $n \to \infty$ for all fixed $x$. Hence, we get
	\begin{align*}
	e^x = \sum_{m=0}^\infty \frac {x^m}{m!}
	\end{align*}
	which is the infinite Maclaurin series for the exponential function.
	\item
	In our last example the approximation worked for all $x \in \R$. However, in some cases $R_n(x) \to 0$ as $n \to \infty$ works for a range of $x$ values but not all of them.
	For this purpose regard the geometric series:
	\begin{align*}
	f(x) = \frac 1 {1-x}
	\end{align*}
	The respective Maclaurin series expanded at $a=0$ is
	\begin{align*}
		\frac 1 {1-x} & = 1 + x + x^2 + \dots + x^n + R_n(x) \\
		R_n(x) & = \frac 1 {1-x} - \left( 1 +x + x^2 + \dots + x^n \right) \\
			& = \frac 1 {1-x} - \frac{1-x^{n+1}}{1-x} = \frac{x^{n-1}}{1-x}
	\end{align*}
	If $-1 < x < 1$, $R_n(x) \to 0$ as $n \to \infty$ but if $x \ge 1$ or $x \le -1$, then $\lim_{x \to \infty} R_n(x)$ is undefined.
	\begin{align*}
		\frac{1}{1-x} = 1+ x + x^2 + x^3 + \dots
	\end{align*}	
	is only valid for $-1<x<1$.
	\item
	Consider the binomial expansion
	\begin{align*}
	f(x) = (1+x)^p
	\end{align*}
	where $p$ is constant. Expand this about $a=0$.
	\begin{align*}
	f^{(1)}(x) & = p(1+x)^{p-1} \\
	f^{(2)}(x) & = p(p-1)(1+x)^{p-2} \\
	& \dots \\
	f^{(m)}(x) & = p(p-1)\dots (p-m+1)(1+x)^{p-m}
	\end{align*}
	The Maclaurin series is
	\begin{align*}
	f(x) & = 1 + px+ \frac{p(p-1)}{2!} x^2 + \dots + \frac{p(p-1) \dots (p-n+1)}{n!} x^n + R_n(x) \\
	R_n(x) & = \frac{f^{(n+1)}(c)}{(n+1)!} x^{n+1} \\
	& = \frac{p(p-1) \dots (p-n) (1+c)^{p-n} }{(n+1)!} x^{n+1}, \qquad 0 \le c \le x
	\end{align*}
	We claim that $R_n(x) \to 0$ as $x \to \infty$ if $-1<x<1$.
	If $x> 0$, $c$ between 0 and $x$. If $x>0$ then $1+c<1$. The solution in this case is to use the Cauchy form of the remainder for negative $x$:
	\begin{align*}
	R_n(x) & = \frac{f^{(n+1)}(c)}{n!} x(x-c)^n \\
	& = \frac{p(p-1) \dots (p-n)}{n!} (1+c)^{p-n} (x-c)^n x 
	\end{align*}
	The fraction is a constant and hence unimportant for the limit as $x \to \infty$.
	\begin{align*}
	(1+x)^{p-n} (x-c)^n & = (1+c)^p \left( \frac{x-c}{1+c} \right)^n \\
	& = (1+c)^p  \left( \frac{x-c}{(c-x)+(1+x)} \right) ^n \\
	& \le (1+c)^p  \left( \frac{-1}{1+(1+x)} \right) ^n 
	\end{align*}
	Because $1> c-x > 0$. This has the limit 0 as $x \to \infty$ 
\end{itemize}
\end{ex}



\subsection{Manipulating Infinite Maclaurin Series}
We can multiply and compose infinite power series.
\begin{ex}
	\begin{itemize}
	\item
	Assume we wanted to calculate the first three nonzero terms of the power series for $\tan x$:
	\begin{align*}
	\tanh x & = \frac{\sinh x}{\cosh x} \\
	& = \left( x+ \frac{x^3}{3!} + \frac{x^5}{5!} + \dots \right) \cdot \left( 1 + \underbrace{\frac{x^2}{2!} + \frac{x^4}{4!} + \dots}_d \right) ^{-1} \tag{1} \\
	& = \left( x+ \frac{x^3}{3!} + \frac{x^5}{5!} + \dots \right) \cdot x \left( 1 - \left( \frac{x^2}{2!} + \frac{x^4}{4!} + \dots \right) + \left( \frac{x^2}{2!} + \frac{x^4}{4!} + \dots \right)^2 - \dots \right) \\	
	& = \left( x + \frac{x^3}{3!} + \frac{x^5}{5!} + \dots \right) \left( 1 + x^2 \left( -\frac 1 2\right) + x^4 \left( - \frac 1 {24} + \frac 1 4 \right) + \dots  \right) \tag{2}
	\end{align*}
	(1) holds because of the general binomial theorem
	\begin{align*}
		(1+d)^{-1} = 1 -d + d^2 -d^3 + \dots 
	\end{align*}
	The 2nd bracket of (2) can be simplified to
	\begin{align*}
		 1- \frac 1 2 x^2 + \frac 5 {24} + \dots
	\end{align*}
	Hence the product of the 2 brackets of (2) is:
	\begin{align*}
		 & x^3 \left( \frac 1 6 - \frac 1 2 \right) + x^5 \l + \left( \frac 3 {100} - \frac 1 {12} + \frac 5 {24} \right) + \dots \\
		 = & x - \frac 1 3 x^3 + \frac 2 {15} x^5 + \dots 		
	\end{align*}
	In order to get to this result, we could have also used 
	\begin{align*}
	f(x) = f(0) + f''(0) x + \frac{f''(0) x^2}{2!} + \dots + \frac{f^{(5)}(0) x^5}{5!}
	\end{align*}
	\item
	Find the first two nonzero terms im the Maclaurin series of $\log(\cos x)$.
	\begin{align*}
	\log(\cos x) & = \log \left( 1- \frac{x^2}{2!} + \frac{x^4}{4!} + \dots \right) \\
	& = \left( - \frac{x^2}{2!} + \frac{x^4}{4!} + \dots \right) - \frac 1 2 \left( - \frac{x^2}{2!} + \frac{x^4}{4!} + \dots \right) ^2 + \dots
	\end{align*}
	because
	\begin{align*}
	\log(1+X) = X - \frac {X^2} 2 + \frac{X^3} 3 - \dots 
	\end{align*}
	where in our case
	\begin{align*}
	X =  - \frac{x^2}{2!} + \frac{x^4}{4!} + \dots
	\end{align*}
	So we get 
	\begin{align*}
	\log(\cos x) & = 0 - \frac 1 2 x^2 + x^4 \left( \frac 1 {24} - \frac 1 8 \right) + \dots \\
	& = - \frac 1 2 x^2 - \frac 1 {12} x^4 + \dots 
	\end{align*}
\end{itemize}
\end{ex}
We can integrate and differentiate power series
\begin{ex}
	\begin{itemize}
		\item
		Through differentiating the power series it is possible to calculate the first derivative of the hyperbolic tan function:
		\begin{align*}
		\tanh^{-1} & = x + \frac{x^3} 3 + \frac{x^5} 5 + \frac{x^7} 7 \dots \\
		\frac d {dx} \tanh^{-1} x & = 1+ x^2 + x^4 + x^6 + \dots \\
		& = \frac{1}{1-x^2} 
		\end{align*}
		\item
		Similarly we can obtain the well-known first derivative of $\sin x$:
		\begin{align*}
		\frac{d}{dx} \sin x & = \frac{d}{dx} \left( x - \frac{x^3}{3!} + \frac{x^5}{5!} - \dots \right) \\
		& = 1- x - \frac{x^2}{2!} + \frac{x^4}{4!} - \dots \\
		& = \cos x 
	\end{align*}
\end{itemize}
\end{ex}
We argued that for certain infinite Taylor series the remainder term is absent (vanishes in the $n \to \infty$ limit).
The reverse procedure also works. We can define functions as power series without a remainder term.
	
Take as a starting point 
\begin{align*}
 f(x) = \sum_{m= 0}^\infty c_m x^m, \qquad c_0, c_1, c_2, \dots \in \R
\end{align*}
For $c_m = \frac 1 {m!}$ we get the exponential function. Taking $c_m = 1$ for all $m$ gives us
\begin{align*}
 f(x) = \frac 1 {1-x}
\end{align*}
The problem is that we can always define a function this way but we don't know whether it is convergent if the sum is convergent.

\begin{ex}
	\begin{itemize}
		\item
	$c_m = \frac 1 m$ gives us $f(x) = e^x$. This formula is valid for any $x$
	\item
	The geometric series:
	\begin{align*}
		c_m = 1 \qquad f(x) = \frac 1 {1-x}
	\end{align*}
	This expansion is only valid if $-1<x<1$.
	\item
	An extreme example is
	$c_m = m!$.
	\begin{align*}
		 f(x) = \sum_{m=0}^\infty m! x^m
	\end{align*}
	This only converges if $x = 0$.
	\item
	For the infinite sum
	\begin{align*}
	c_m = \frac 1 {(m!)^2} \qquad f(x) = \sum_{m=0}^\infty \frac{x^m}{(m!)^2}
	\end{align*}
	we do not know for which $x$ it converges.
\end{itemize}
\end{ex}

We would like to know under what conditions 
\begin{align*}
f(x) = \sum_{m=0}^\infty c_m x^m = \lim_{n \to \infty} \sum_{m=0}^n c_m x^m
\end{align*}
converges and if so for what range of $x$.
	
For this purpose we have to take a step back. Consider convergence of numerical series. That means a sum of the form 
\begin{align*}
\sum_{m=0}^\infty a_m
\end{align*}
where $a_0,a_1, a_2, \dots $ is an infinite list of numbers.

\begin{ex}
	\begin{itemize}
		\item
		Consider
		\begin{align*}
		a_m & = \frac 1 {m^4}, \qquad m \ge 1 \\
		a_0 & = 0 \\
		\sum_{m=1}^\infty \frac 1 {m^4} & = 1 + \frac 1 {2^4} + \frac 1 {3^4} + \frac 1 {4^4} = \xi(4) = \frac{\pi^4}{90}
		\end{align*}
		is a convergent series.
		\item The harmonic series:
		\begin{align*}
		a_m = \frac 1 m, \qquad m \ge 1, a_0 = 0 \\
		\sum_{m=1}^\infty \frac 1 m = 1 + \frac 1 2 + \frac 1 3 + \frac 1 4 \dots
		\end{align*}
		does not converge. 
		\item The alternating harmonic series:
		\begin{align*}
		\sum_{m=1}^\infty \frac{(-1)^{m+1}}{m} & = 1 - \frac 1 2 + \frac 1 3 - \frac 1 4 + \dots \\
		& = \log 2
		\end{align*}
		\item
		The following series oscillates between 1 and -1 but does not have a limit for $n \to \infty$
		\begin{align*} 
		& \sum_{m=0}^\infty (-1)^m, \qquad a_m = (-1)^m \\
		= & 1 -1 +1 -1 +1 \dots \\
		= & \sum_{m=0}^n (-1)^m = \frac{(-1)^n+1}{2} 
		\end{align*}
	\end{itemize}
\end{ex}

How to decide whether a numerical series converges?
\begin{itemize}
	\item
	Evaluate it! 
	\item
	Use some standard tests.
\end{itemize}

\subsection{Tests}
\begin{enumerate}
	\item
	\emph{Preliminary Test (Easy Test)} \\
	If $a_m \not \to 0$ as $m \to \infty$ then 
	\begin{align*}
	\sum_{m=0}^\infty a_m 
	\end{align*}
	does not converge.
	If the series converges, then $a_m \to 0$ as $m \to \infty$.
	\begin{ex}
		\begin{itemize}
			\item
			$\displaystyle
			\sum_{m=1}^\infty \frac 1 {m^4} \qquad \text{and} \qquad  \sum_{m=1}^\infty \frac 1 {m}
			$ \\
		both pass the preliminary test since $a_m \to 0$ as $m \to \infty$.
		\item
		$\displaystyle
			\sum_{m=0}^\infty = 0+1+2+3+4+ \dots
		$ \\
		does not pass the preliminary test and hence diverges.
	\end{itemize}
	\end{ex}
	\item
	\emph{Alternating Series Test} \\
	Suppose $a_m$ is alternating and $|a_m|$ is strictly decreasing, $|a_m+1| < |a_m|$ and $a_m$ passes the preliminary test, then 
	\begin{align*}
	\sum_m a_m
	\end{align*}
	is convergent.
	\begin{ex} Alternating Harmonic series.
		\begin{align*}
			s_m & = 1- \frac 1 2 + \frac 1 3 - \frac 1 3 + \frac 1 5 - \dots \\
			a_m & = \frac{(-1)^{m+1}}{m} \qquad m \ge 1 \\
			\lim_{m \to \infty} s_m & = \ln 2
		\end{align*}
	\end{ex}
\end{enumerate}

\begin{df}\emph{Absolute Convergence} \\
If $\displaystyle \sum_m |a_m|$ converges, then the series $\displaystyle \sum_m a_m$ is said to be absolutely convergent.
\end{df}

\begin{rk}
	Absolute convergence is stronger than convergence. I.e. every absolutely convergent series is convergent but not vice versa.
\end{rk}

\begin{ex}
	The alternating harmonic series is convergent but not absolutely convergent.
\end{ex}

\subsection{Tests for Absolute Convergence}
\begin{enumerate}
	\item
	\emph{Comparison test} \\
	Suppose $\displaystyle |a_m| \le |b_m|$ and $\displaystyle \sum_m b_m$ is absolutely convergent. Then so is $ \displaystyle \sum_m a_m$.
	
	\begin{ex} The series
		\begin{align*}
		\sum_{m=0}^\infty \frac 1 {m^2+e^{-m}}  
		\end{align*}
		converges because
		\begin{align*}
		\frac 1 {m^2+ e^{-m}} < \frac 1 {m^2} 
		\end{align*}
		and this series converges to $\frac{\pi^2} 6$.
	\end{ex}
	\item
	\emph{Integral Test}\\
	Suppose $|a_m| = f(m)$ where $f$ is a decreasing positive function. Consider the integral
	\begin{align*}
	I = \int_N^\infty f(x) dx.
	\end{align*}
	If $I$ diverges for any $N$, then the series $\sum_m a_m$ is not absolutely convergent and if $I$ exists or is finite for some $N$, then the series is absolutely convergent.
	
	
	\begin{ex}
		\begin{itemize}
			\item
			The series
			\begin{align*}
			\sum_{m=1}^\infty \frac 1 m & \\
			\int_n^\infty \frac x {dx} & = [\log x ]^{x= \infty}_{x =N} = \log \infty - \log N
			\end{align*}
			is meaningless or infinite. This shows once again that
			\begin{align*}
			\sum_{m=1}^\infty \frac 1 m
			\end{align*}
			diverges.
			\item
			\begin{align*}
			\sum_{m=1}^\infty \frac 1 {m^2} & \\
			\int_N^\infty \frac 1 {x^2} dx & = \left[- \frac 1 x \right]^{x=\infty}_{x=N} \\
			& = 0 + \frac 1 N = \frac 1 N
			\end{align*}
			is finite unless $N=0$. By the integral test, the sum
			\begin{align*}
			\sum_{m=1}^\infty \frac 1 {m^2}
			\end{align*}
			is absolutely convergent.
		\end{itemize}
	\end{ex}
	\item
	\emph{Ratio and root tests} \\
	The idea is to approximate a series $\sum_m a_m$ with a geometric series. \\
	\emph{Ratio test} \\
	Consider
	\begin{align*}
	L = \lim_{n\to \infty} \left| \frac{a_{m+1}}{a_m} \right|
	\end{align*}
	\emph{Root test} \\
	Consider 
	\begin{align*}
	L = \lim_{m \to \infty} |a_m|^{\frac 1 m}
	\end{align*}
	Using either test, if $L>1$ the series diverges. if $L<1$ the series is absolutely convergent. If $L = 1$ the test is indecisive.
	
	\begin{ex} Consider
		\begin{align*}
		\sum_{m=0}^\infty m e^{-m} \\
		a_m = me^{-m}
		\end{align*}
		converges due to exponential decay. The ratio is
		\begin{align*}
		\left| \frac{a_{m+1}}{a_m} \right| & = \frac{(m+1)e^{m+1}}{m e^{-1}} \\
		& = \frac{m+1}{m} e^{-1} \\
		& \overset{m \to \infty}{\to} \frac 1 e < 1.
		\end{align*}
		Hence, the series converges due to ratio test. The root test gives us
		\begin{align*}
		|a_m|^{\frac 1 m} & = (m e^{-m})^{\frac 1 m} \\
		& = m^{\frac 1 m} e^{-1} \\
		& = e^{\frac 1 m \log m} e^{-1} \\
		& \overset{m\to \infty}{\to} e^0 e^{-1} = \frac 1 e.
		\end{align*}
		By the root test, the series is convergent.
	\end{ex}
	In both cases $L= \frac 1 e < 1$. The ratio and root test are comparing the series with the geometric series
	\begin{align*}
	\sum_m L^m
	\end{align*}
	where $L$ is a constant. If $L>1$, the series is not convergent.
\end{enumerate}




\begin{ex}
	\begin{itemize}
		\item
	exponential series
	\begin{align*}
	e^x = \sum_{m=0}^\infty \frac{x^m}{m!}
	\end{align*}
	converges for all $x$.
	\begin{align*}
	a_m = \frac{x^m}{m!}
	\end{align*}
	The ratio test gives us
	\begin{align*}
	\left| \frac{a_{m+1}}{a_m} \right| & = \frac{\frac{|x|^{m+1}}{(m+1)!}}{\frac{|x|^m}{m!}} \\
		& = \frac{m!|x|}{(m+1)!} \\
		& = \frac{|x|}{m+1} \\
		& \overset{m \to \infty}{\to} 0.
	\end{align*}
	The same can be done for the root test:
	\begin{align*}
	|a_m|^{\frac 1 m} = \frac{|x|}{(m!)^{\frac 1 m}}
	\end{align*}
	We need to consider $(m!)^{\frac 1 m}$ for large $m$. This can be done using Stirling's approximation:
	\begin{align*}
	n! \approx \sqrt{2 \pi n} \left( \frac n e \right) ^n 
	\end{align*}
	describes the behaviour of factorial for large numbers.
	\begin{align*}
	(m!)^{\frac 1 m} & \approx (2 \pi m)^{\frac 1 {2m}} \frac m e \\
	& = e^{1}{2m} \log(2 \pi m) \\
	& \overset{m \to \infty}{\to} 1
	\end{align*}
	Hence, $L=0$ and by the root test the series converges for all $x$.
	\end{itemize}
\end{ex}

\emph{Claim} \\
Any power series $\sum_m c_m x$ converges for all $x$ or converges for $|x|<R$ and diverges for $|x|>R$ where $R$ is a non-negative constant (which is different for each power series and is called radius of convergence). If series converges for all $x$ we can write $R = \infty$.

\begin{ex}
	\begin{itemize}
		\item
		$ \displaystyle
		\sum_{m=0}^\infty 2^m m x^m 
		$ \\
		To find $R$ for this series use the ratio test:
		\begin{align*}
		a_m & = 2^m m x^m \\
		\left| \frac{a_{m+1}}{a_m} \right| & = \frac{2^{m+1}(m+1) |x|^{m+1}}{2^m m |x|^m} \\
		& = \frac{2(m+1)} m |x| \\
		& \overset{m \to \infty}{\to} 2|x| \\
		L & = 2|x|  
		\end{align*}
		This is smaller than 1 for $x < \frac 1 2$ so the series converges for $-\frac 1 2 < x < \frac 1 2$ and $R= \frac 1 2$.
		\item
		\begin{align*}
		\sin^{-1} x & = \sum_{p=0}^\infty \frac{4^{-p}(2p)! x^{2p+1}}{(2p+1)(p!)^2} \\
		a_p & = \frac{4^{-p}(2p)! x^{2p+1}}{(2p+1)(p!)^2}
		\end{align*}
		The ratio test results in 
		\begin{align*}
		\left| \frac{a_{p+1}}{a_p} \right| & = \frac{4^{-(p+1)}}{4^{-p}} \cdot \frac{(2p+2)!}{(2p)!} \cdot \frac{(p!)^2}{((p+1)!)^2} \cdot \frac{2p+1}{2p+3} |x|^2 \\
		& = \frac 1 4 ( 2p+1) (2p+2) \frac 1 {(p+1)^2} \cdot \frac{2p+1}{2p+3} |x|^2 \\
		& \overset{p \to \infty}{\to} \left|x^2 \right|
		\end{align*}
		For convergence $|x|^2<1$ hence $R=1$. 
	\end{itemize}
\end{ex}
The root test is not as convenient as the ratio test in computing $R$ but we can use the root test to obtain a formula for $R$.
\begin{align*}
& \sum_m c_m x^m \\
a_m & = c_m x^m \\
& \overset{m \to \infty}{\to} |x| \cdot \lim_{m\to\infty} |c_m|^{\frac 1 m} \\
R & = \frac 1 {\lim_{m \to\infty} |c_m|^{\frac 1 m}}
\end{align*}
works if the limit exists. Otherwise the formula is obviously wrong. In this case take the \emph{Hadamard formula}:
\begin{align*}
R = \frac 1 {\lim \sup |c_m|^{\frac 1 m}}
\end{align*} 
$\lim\sup$ is called the limit superior.
\begin{align*}
\lim \sup a_n = \lim_{m\to\infty} b_m \\
b_m = \underset{n \ge m}{\sup} a_n
\end{align*}

\begin{ex}
	\begin{align*}
	c_m & = \left\{
	\begin{array}{l l}
	1, & m~ \text{prime} \\
	0, & \text{otherwise}
	\end{array} \right. \\
	f(x) & = x^2 + x^3 + x^5+ x^7 +x^{11} + \dots \\
	R & = 1
	\end{align*}
\end{ex}


\begin{df}
	Consider
	\begin{align*}
	\sum_{m = 0}^\infty c_m z^m
	\end{align*}
	where $c_0, c_1, c_2, \dots$ is an infinite list of complex numbers $z = x + iy$.
\end{df}

\emph{Claim.} \\
A complex power series converges for $|z| < R$ where $R$ is a non-negative constant (radius of convergence).


If a complex function is analytic in a disc, the complex Taylor series always converges within the disc.
\\

Idea: Consider the complex function
\begin{align*}
f(z) & = \frac 1 {e^2+1}, \qquad z = x+yi \\
& = \sum_{m=0}^\infty c_m z^m
\end{align*}
We don't know what the coefficients $c_m$ are. We find out where $f(z)$ is singular in the complex plane.
Recipe: $R$ is the distance between the origin and the nearest singularity in the complex plane. This results in $R= \pi$.

\subsection{L'H\^opital's Rule}
L'H\^opital's Rule is a formula for indeterminate limits:
\begin{align*}
\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}
\end{align*} 
if
\begin{align*}
\lim_{x \to a} f(x) = \lim g(x) = 0
\end{align*}

\begin{ex}
	\begin{itemize}
		\item
		\begin{align*}
		\lim_{x \to 1} \frac{\cos(\frac \pi 2 x)}{1-x^2} & = \lim_{x \to 1} \frac{- \frac \pi 2 \sin\frac{\pi x} 2}{-2x} \\
		& = \frac{- \frac \pi x}{-2} = \frac \pi 4
		\end{align*}
		\item
		\begin{align*}
		\lim_{x \to 0} \frac{\sin^2 x}{x^2} & = \lim_{x\to 0} \frac{2 \sin x \cos x}{2x} \\
		& = \lim_{x \to 0} \frac{2 \sin 2x}{2x} \\
		& = \lim_{x \to 0} \frac{2 \cos 2x}{2} = 1
		\end{align*}
	\end{itemize}
\end{ex}

Justification:
\begin{itemize}
	\item
	Applied maths approach:
	Consider the Taylor series for $f(x)$ and $g(x)$ expanded about $x=a$.
	\begin{align*}
	\frac{f(x)}{g(x)} = \frac{f(a) + f'(a) (x-a) + \frac 1 {2!} f''(a) (x-a)^2 + \dots}{g(a) +g'(x)(x-a) + \frac 1 {2!}  g''(a) (x-a)^2+ \dots}
	\end{align*}
	What is the limit for $x \to a$? If $g(a) \neq 0$ the limit is
	\begin{align*}
	\frac{f(a)}{g(a)}.
	\end{align*}
	Suppose $g(a) = f(a) = 0$:
	\begin{align*}
	\frac{f(x)}{g(x)} & = \frac{f'(a) + f''(a) (x-a) + \frac 1 {2!} f'''(a) (x-a)^2 + \dots}{g'(a) +g''(x)(x-a) + \frac 1 {2!}  g'''(a) (x-a)^2+ \dots} \\
	& \overset{x \to a}{\to} \frac{f'(x)}{g'(x)}
	\end{align*}
	If $g'(a) \neq 0$. If $g'(a) = f'(a) = 0$, limit is
	\begin{align*}
	\frac{f''(a)}{g''(a)}
	\end{align*}
	if $g''(a) \neq 0$.
	\item
	Pure maths prove
	\begin{align*}
	\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f(x) - f(a)}{g(x)-g(a)}
	\end{align*}
	Apply Cauchy's MVT:
	\begin{align*}
	\frac{f(x) - f(a)}{g(x)-g(a)} & = \frac{f'(c)}{g(c)} \\
	\dots& 
	\end{align*}
\end{itemize}









